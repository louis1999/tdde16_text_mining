{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic pre processing + count vectorizer + Kmeans      With cross val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"train.csv\", lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20800, 5)\n",
      "(18843, 5)\n",
      "(18285, 5)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "\n",
    "df.dropna(subset = [\"author\"], inplace=True) # removing the nan rows for author\n",
    "print(df.shape)\n",
    "\n",
    "df.dropna(subset = [\"title\"], inplace=True) # removing the null rows for title\n",
    "\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.compose import make_column_transformer # for vectorizing multiple columns at the same time\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X = df[[\"title\", \"author\", \"text\"]]\n",
    "\n",
    "\n",
    "ct = make_column_transformer((vectorizer, \"title\"), (vectorizer, \"author\"), (vectorizer, \"text\") )\n",
    "\n",
    "\n",
    "y = df.iloc[:, 4:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, n_init=2)\n",
    "clf = make_pipeline(ct, kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.21      0.34      1031\n",
      "           1       0.49      0.97      0.65       798\n",
      "\n",
      "    accuracy                           0.54      1829\n",
      "   macro avg       0.70      0.59      0.49      1829\n",
      "weighted avg       0.72      0.54      0.47      1829\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.19      0.32      1017\n",
      "           1       0.49      0.98      0.65       812\n",
      "\n",
      "    accuracy                           0.54      1829\n",
      "   macro avg       0.70      0.58      0.49      1829\n",
      "weighted avg       0.72      0.54      0.47      1829\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.81      0.65      1081\n",
      "           1       0.07      0.02      0.03       748\n",
      "\n",
      "    accuracy                           0.49      1829\n",
      "   macro avg       0.31      0.42      0.34      1829\n",
      "weighted avg       0.35      0.49      0.40      1829\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.20      0.33      1047\n",
      "           1       0.48      0.97      0.64       782\n",
      "\n",
      "    accuracy                           0.53      1829\n",
      "   macro avg       0.69      0.59      0.49      1829\n",
      "weighted avg       0.72      0.53      0.46      1829\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.79      0.62      1018\n",
      "           1       0.08      0.02      0.04       811\n",
      "\n",
      "    accuracy                           0.45      1829\n",
      "   macro avg       0.29      0.41      0.33      1829\n",
      "weighted avg       0.32      0.45      0.36      1829\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.80      0.62      1015\n",
      "           1       0.10      0.03      0.05       813\n",
      "\n",
      "    accuracy                           0.46      1828\n",
      "   macro avg       0.31      0.41      0.33      1828\n",
      "weighted avg       0.33      0.46      0.36      1828\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.20      0.32      1044\n",
      "           1       0.48      0.97      0.64       784\n",
      "\n",
      "    accuracy                           0.53      1828\n",
      "   macro avg       0.68      0.58      0.48      1828\n",
      "weighted avg       0.71      0.53      0.46      1828\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.19      0.32      1074\n",
      "           1       0.46      0.99      0.63       754\n",
      "\n",
      "    accuracy                           0.52      1828\n",
      "   macro avg       0.71      0.59      0.47      1828\n",
      "weighted avg       0.75      0.52      0.45      1828\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.21      0.34      1032\n",
      "           1       0.49      0.98      0.65       796\n",
      "\n",
      "    accuracy                           0.55      1828\n",
      "   macro avg       0.72      0.60      0.50      1828\n",
      "weighted avg       0.75      0.55      0.48      1828\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.79      0.61      1002\n",
      "           1       0.06      0.02      0.02       826\n",
      "\n",
      "    accuracy                           0.44      1828\n",
      "   macro avg       0.28      0.40      0.32      1828\n",
      "weighted avg       0.30      0.44      0.35      1828\n",
      "\n",
      "[0.54127939 0.54018589 0.48879169 0.53198469 0.4516129  0.45623632\n",
      " 0.52789934 0.51969365 0.54595186 0.44201313]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, accuracy_score, make_scorer\n",
    "# Code from : https://stackoverflow.com/questions/42562146/classification-report-with-nested-cross-validation-in-sklearn-average-individua\n",
    "def classification_report_with_accuracy_score(y_true, y_pred):\n",
    "    print(classification_report(y_true, y_pred)) # print classification report\n",
    "    return accuracy_score(y_true, y_pred) # return accuracy score\n",
    "\n",
    "scores = cross_val_score(clf, X, y, cv=10,scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.504564887605835\n",
      "0.00151559931189521\n",
      "0.038930698836460796\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.average(scores))\n",
    "print(np.var(scores))\n",
    "print(np.std(scores))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.567\n",
      "0.5050000000000001\n"
     ]
    }
   ],
   "source": [
    "#precision\n",
    "import numpy\n",
    "print(numpy.average([0.72, 0.72, 0.35, 0.72, 0.32, 0.33, 0.71, 0.75, 0.75, 0.30]))\n",
    "# recall\n",
    "print(numpy.average([0.54, 0.54, 0.49, 0.53, 0.45, 0.46, 0.53, 0.52, 0.55, 0.44]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
